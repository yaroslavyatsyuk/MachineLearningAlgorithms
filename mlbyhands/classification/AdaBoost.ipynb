{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Algorithm`\n",
    "\n",
    "1\\. Start with same weight for all data  $\\alpha_j = 1/N$\n",
    "\n",
    "2\\. For t = 1,...T:\n",
    "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "  * Compute coefficient $\\hat{w}_t$:\n",
    "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "  * Re-compute weights $\\alpha_j$:\n",
    "     $$\\alpha_j \\gets \\begin{cases}\n",
    "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
    "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
    "     \\end{cases}$$\n",
    "  * Normalize weights $\\alpha_j$:\n",
    "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier:\n",
    "\n",
    "    def __init__(self, num_tree_stumps=30, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.num_tree_stumps = num_tree_stumps\n",
    "        self.n = None\n",
    "        self.features = None\n",
    "        self.tree_weights = None\n",
    "        self.trees = None\n",
    "\n",
    "    def fit(self, data, features, target):\n",
    "        self.n = len(data)\n",
    "        self.features = features\n",
    "\n",
    "        self.tree_weights, self.trees = \\\n",
    "            self._train_ada_boost(data, features, target, self.num_tree_stumps, self.max_depth)\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = np.zeros(len(X))\n",
    "\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            predictions = X.apply(lambda x: self._predict(tree, x), axis=1)\n",
    "            scores += predictions * self.tree_weights[i]\n",
    "\n",
    "        return np.array([1 if score > 0 else 0 for score in scores])\n",
    "\n",
    "    def _train_ada_boost(self, data, features, target, num_tree_stumps, tree_depth):\n",
    "\n",
    "        N = len(data)\n",
    "        alpha = np.ones(N) / N\n",
    "        weights = []\n",
    "        tree_stumps = []\n",
    "        # Convert labels to AdaBoost format (1 and -1)\n",
    "        target_values = data[target].apply(lambda y: 1 if y > 0 else -1)\n",
    "\n",
    "        for t in range(num_tree_stumps):\n",
    "            print('AdaBoost Iteration %d' % t)\n",
    "            tree_stump = self._train_tree(data, features, target, data_weights=alpha, max_depth=tree_depth)\n",
    "            tree_stumps.append(tree_stump)\n",
    "\n",
    "            predictions = data.apply(lambda x: self._predict(tree_stump, x), axis=1)\n",
    "\n",
    "            is_correct = (predictions == target_values)\n",
    "            # Compute weighted error\n",
    "            weighted_error = np.sum(alpha[predictions != target_values]) / np.sum(alpha)\n",
    "\n",
    "            # Compute model coefficient using weighted error\n",
    "            weight = 0.5 * np.log((1 - weighted_error) / weighted_error)\n",
    "            weights.append(weight)\n",
    "\n",
    "            # Adjust weights on data point and scale alpha by multiplying by adjustment\n",
    "            # Then normalize data points weights\n",
    "            alpha *= is_correct.apply(lambda y: np.exp(-weight) if y else np.exp(weight))\n",
    "            alpha = alpha / np.sum(alpha)\n",
    "\n",
    "        return weights, tree_stumps\n",
    "\n",
    "    def _calculate_gini(self, labels_in_node, data_weights):\n",
    "        if len(labels_in_node) == 0:\n",
    "            return 0\n",
    "\n",
    "        node_wght = np.sum(data_weights)\n",
    "        \n",
    "        first_class_wght = np.sum(data_weights[labels_in_node == 1])\n",
    "        second_class_wght = np.sum(data_weights[labels_in_node == 0])\n",
    "\n",
    "        # Gini = 1 - sum(Pr_c^2)\n",
    "        return 1 - ((first_class_wght/node_wght)**2 + (second_class_wght/node_wght)**2)\n",
    "\n",
    "    def _calculate_gini_and_get_best_split_threshold(self, data, data_weights, feature, target):\n",
    "        unique_inputs = data[feature].unique()\n",
    "        if len(unique_inputs) > 100:\n",
    "            unique_inputs = np.quantile(unique_inputs, np.arange(0, 1.01, 0.025))\n",
    "\n",
    "        N = np.sum(data_weights)\n",
    "        best_gini = float(\"inf\")\n",
    "        best_threshold = None\n",
    "\n",
    "        for val in unique_inputs:\n",
    "            left_split = data[data[feature] < val]\n",
    "            right_split = data[data[feature] >= val]\n",
    "\n",
    "            L_wgh = data_weights[data[feature] < val]\n",
    "            R_wgh = data_weights[data[feature] >= val]\n",
    "\n",
    "            N_L = np.sum(L_wgh)\n",
    "            N_R = np.sum(R_wgh)\n",
    "\n",
    "            gini_L = self._calculate_gini(left_split[target], L_wgh)\n",
    "            gini_R = self._calculate_gini(right_split[target], R_wgh)\n",
    "\n",
    "            gini_for_current_val = (N_L / N) * gini_L + (N_R / N) * gini_R\n",
    "\n",
    "            if gini_for_current_val < best_gini:\n",
    "                best_threshold = val\n",
    "                best_gini = gini_for_current_val\n",
    "\n",
    "        return best_gini, best_threshold\n",
    "\n",
    "    def _find_best_splitting_feature(self, data, data_weights, features, target):\n",
    "        best_feature = None\n",
    "        best_split_threshold = None\n",
    "        best_gini = float(\"inf\")\n",
    "\n",
    "        # Loop through each feature to consider splitting on that feature\n",
    "        for feature in features:\n",
    "            current_feature_gini, threshold = \\\n",
    "                self._calculate_gini_and_get_best_split_threshold(data, data_weights, feature, target)\n",
    "\n",
    "            if current_feature_gini < best_gini:\n",
    "                best_feature = feature\n",
    "                best_gini = current_feature_gini\n",
    "                best_split_threshold = threshold\n",
    "\n",
    "        return [best_feature, best_gini, best_split_threshold]\n",
    "\n",
    "    def _predict(self, tree, x):\n",
    "        if tree[\"is_leaf\"]:\n",
    "            return tree[\"prediction\"]\n",
    "        else:\n",
    "            split_feature_value = x[tree['splitting_feature']]\n",
    "            threshold = tree[\"split_threshold\"]\n",
    "            if split_feature_value < threshold:\n",
    "                return self._predict(tree[\"left\"], x)\n",
    "            else:\n",
    "                return self._predict(tree[\"right\"], x)\n",
    "\n",
    "    def _calculate_node_weighted_mistakes(self, labels_in_node, data_weights):\n",
    "        total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "        weighted_mistakes_all_negative = total_weight_positive\n",
    "\n",
    "        total_weight_negative = sum(data_weights[labels_in_node == 0])\n",
    "        weighted_mistakes_all_positive = total_weight_negative\n",
    "\n",
    "        return (weighted_mistakes_all_positive, +1) if \\\n",
    "            weighted_mistakes_all_positive <= weighted_mistakes_all_negative else \\\n",
    "            (weighted_mistakes_all_negative, -1)\n",
    "\n",
    "    def _create_leaf(self, target_values, data_weights):\n",
    "        leaf = {'splitting_feature': None,\n",
    "                'is_leaf': True}\n",
    "\n",
    "        # Computed weight of mistakes.\n",
    "        weighted_error, best_class = self._calculate_node_weighted_mistakes(target_values, data_weights)\n",
    "\n",
    "        leaf['prediction'] = best_class\n",
    "        return leaf\n",
    "\n",
    "    def _create_node(cls, splitting_feature, split_threshold, left_tree, right_tree):\n",
    "        return {\"is_leaf\": False,\n",
    "                \"prediction\": None,\n",
    "                \"splitting_feature\": splitting_feature,\n",
    "                \"split_threshold\": split_threshold,\n",
    "                \"left\": left_tree,\n",
    "                \"right\": right_tree}\n",
    "\n",
    "    def _train_tree(self, data, features, target, data_weights, current_depth=1, max_depth=1):\n",
    "        remaining_features = features[:]\n",
    "        target_values = data[target]\n",
    "\n",
    "        # Stopping condition 1. Error is 0.\n",
    "        if self._calculate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "            return self._create_leaf(target_values, data_weights)\n",
    "\n",
    "        # Stopping condition 2: No more features to split on.\n",
    "        if not remaining_features:\n",
    "            return self._create_leaf(target_values, data_weights)\n",
    "\n",
    "        # Early stopping condition 1: Reached max depth limit.\n",
    "        if current_depth > max_depth:\n",
    "            return self._create_leaf(target_values, data_weights)\n",
    "\n",
    "        # Find the best splitting feature\n",
    "        splitting_feature, feature_error, split_threshold = \\\n",
    "            self._find_best_splitting_feature(data, data_weights, features, target)\n",
    "        remaining_features.remove(splitting_feature)\n",
    "\n",
    "        left_split = data[data[splitting_feature] < split_threshold]\n",
    "        right_split = data[data[splitting_feature] >= split_threshold]\n",
    "\n",
    "        left_data_weights = data_weights[data[splitting_feature] < split_threshold]\n",
    "        right_data_weights = data_weights[data[splitting_feature] >= split_threshold]\n",
    "\n",
    "        # Create a leaf node if the split is \"perfect\"\n",
    "        if len(left_split) == len(data):\n",
    "            return self._create_leaf(left_split[target], data_weights)\n",
    "        if len(right_split) == len(data):\n",
    "            return self._create_leaf(right_split[target], data_weights)\n",
    "\n",
    "        # Recurse on left and right subtrees\n",
    "        left_tree = self._train_tree(\n",
    "            left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "        right_tree = self._train_tree(\n",
    "            right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "\n",
    "        return self._create_node(splitting_feature, split_threshold, left_tree, right_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and compare Custom and SKlearn AdaBoost implementations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"target\"] = y\n",
    "data = X\n",
    "# get feature names and target name \n",
    "features = list(data.columns)\n",
    "features.remove(\"target\")\n",
    "target = \"target\"\n",
    "\n",
    "# split into train and test datasets\n",
    "train = data.sample(frac=0.80, random_state=1) \n",
    "test = data.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Iteration 0\n",
      "AdaBoost Iteration 1\n",
      "AdaBoost Iteration 2\n",
      "AdaBoost Iteration 3\n",
      "AdaBoost Iteration 4\n",
      "AdaBoost Iteration 5\n",
      "AdaBoost Iteration 6\n",
      "AdaBoost Iteration 7\n",
      "AdaBoost Iteration 8\n",
      "AdaBoost Iteration 9\n",
      "AdaBoost Iteration 10\n",
      "AdaBoost Iteration 11\n",
      "AdaBoost Iteration 12\n",
      "AdaBoost Iteration 13\n",
      "AdaBoost Iteration 14\n",
      "AdaBoost Iteration 15\n",
      "AdaBoost Iteration 16\n",
      "AdaBoost Iteration 17\n",
      "AdaBoost Iteration 18\n",
      "AdaBoost Iteration 19\n",
      "AdaBoost Iteration 20\n",
      "AdaBoost Iteration 21\n",
      "AdaBoost Iteration 22\n",
      "AdaBoost Iteration 23\n",
      "AdaBoost Iteration 24\n",
      "AdaBoost Iteration 25\n",
      "AdaBoost Iteration 26\n",
      "AdaBoost Iteration 27\n",
      "AdaBoost Iteration 28\n",
      "AdaBoost Iteration 29\n",
      "AdaBoost Iteration 30\n",
      "AdaBoost Iteration 31\n",
      "AdaBoost Iteration 32\n",
      "AdaBoost Iteration 33\n",
      "AdaBoost Iteration 34\n",
      "AdaBoost Iteration 35\n",
      "AdaBoost Iteration 36\n",
      "AdaBoost Iteration 37\n",
      "AdaBoost Iteration 38\n",
      "AdaBoost Iteration 39\n",
      "AdaBoost Iteration 40\n",
      "AdaBoost Iteration 41\n",
      "AdaBoost Iteration 42\n",
      "AdaBoost Iteration 43\n",
      "AdaBoost Iteration 44\n",
      "AdaBoost Iteration 45\n",
      "AdaBoost Iteration 46\n",
      "AdaBoost Iteration 47\n",
      "AdaBoost Iteration 48\n",
      "AdaBoost Iteration 49\n"
     ]
    }
   ],
   "source": [
    "custom_tree_model = AdaBoostClassifier(num_tree_stumps=50, max_depth=1)\n",
    "custom_tree_model.fit(train, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier as AdaBoost\n",
    "from sklearn.tree import DecisionTreeClassifier as DTree\n",
    "\n",
    "sklearn_dtree = DTree(max_depth=1)\n",
    "sklearn_ada_boost = AdaBoost(base_estimator=sklearn_dtree, n_estimators=50)\n",
    "sklearn_ada_boost.fit(train[features], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom model accuracy score on test 0.9736842105263158\n",
      "SKlearn model accuracy score on test 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_custom = accuracy_score(test[target], custom_tree_model.predict(test))\n",
    "accuracy_sklearn = accuracy_score(test[target], sklearn_ada_boost.predict(test[features]))\n",
    "\n",
    "print(\"Custom model accuracy score on test\", accuracy_custom)\n",
    "print(\"SKlearn model accuracy score on test\", accuracy_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_custom - accuracy_sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
